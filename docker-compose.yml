version: "3.9"

services:

  redpanda:
    image: docker.redpanda.com/redpandadata/redpanda:latest
    container_name: redpanda
    restart: unless-stopped
    command:
      - redpanda
      - start
      - --smp
      - "1"
      - --memory
      - 1G
      - --overprovisioned
      - --node-id
      - "0"
      - --kafka-addr
      - PLAINTEXT://0.0.0.0:29092
      - --advertise-kafka-addr
      - PLAINTEXT://redpanda:29092
    volumes:
      - redpanda_data:/var/lib/redpanda/data
    # Kafka fica só na rede interna do Docker (sem publish)

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    container_name: schema-registry
    restart: unless-stopped
    depends_on: [ redpanda ]
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "redpanda:29092"
      SCHEMA_REGISTRY_LISTENERS: "http://0.0.0.0:8081"
    ports:
      - "127.0.0.1:8081:8081"   # Nginx (host) acessa aqui

  redis:
    image: redis:7-alpine
    container_name: redis
    restart: unless-stopped
    command: ["redis-server","--appendonly","yes",
              "--requirepass","${REDIS_PASSWORD}",
              "--bind","0.0.0.0","--protected-mode","no"]
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    volumes:
      - redis_data:/data

  redis-commander:
    image: rediscommander/redis-commander:latest
    container_name: redis-commander
    restart: unless-stopped
    depends_on: [ redis ]
    environment:
      REDIS_HOSTS: "local:redis:6379:0:${REDIS_PASSWORD}"
      # Opcional: exigir login na UI
      # HTTP_USER: admin
      # HTTP_PASSWORD: ${REDIS_COMMANDER_PASSWORD}
    ports:
      - "127.0.0.1:8082:8081"   # evita conflito com 8081

  redpanda-console:
    image: docker.redpanda.com/redpandadata/console:latest
    container_name: redpanda-console
    restart: unless-stopped
    depends_on: [ redpanda ]
    environment:
      KAFKA_BROKERS: "redpanda:29092"
      REDPANDA_ADMIN_URL: "http://redpanda:9644"
      SERVER_LISTEN_ADDRESS: "0.0.0.0:8080"
      # Para servir em subcaminho (/console). Se sua imagem ignorar, use sub_filter no Nginx.
      SERVER_BASEPATH: "/console"
    ports:
      - "127.0.0.1:8080:8080"   # Nginx (host) acessa aqui

  mqtt-kafka-bridge:
    image: iotkafkaacrwtvgek.azurecr.io/mqtt-kafka-bridge:latest
    container_name: mqtt-kafka-bridge
    restart: always
    depends_on: [ redpanda ]
    environment:
      MQTT_BROKER: "broker.hivemq.com"
      MQTT_PORT: 1883
      KAFKA_BROKER: "redpanda:29092"
      KAFKA_TOPIC: "iot-data"

  kafka-redis-consumer:
    image: iotkafkaacrwtvgek.azurecr.io/kafka-redis-consumer:latest
    container_name: kafka-redis-consumer
    restart: always
    depends_on: [ redpanda, redis ]
    environment:
      KAFKA_BROKER: "redpanda:29092"
      KAFKA_TOPIC: "iot-data"
      REDIS_HOST: "redis"
      REDIS_PASSWORD: "${REDIS_PASSWORD}"
    # usa a rede default do compose
    ports:
    # host:contêiner → Nginx falará com 127.0.0.1:8090
    - "127.0.0.1:8090:8000"

  watchtower:
    image: containrrr/watchtower
    container_name: watchtower
    restart: always
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /home/azureuser/.docker/config.json:/config.json:ro
    command: --interval 60 --cleanup mqtt-kafka-bridge kafka-redis-consumer

  nest:
    image: iotkafkaacrwtvgek.azurecr.io/nest:latest
    container_name: nest-api
    restart: always
    depends_on:
      - redis
    environment:
      - NODE_ENV=production
      - PORT=3000
      - JWT_SECRET=asenhafoda123
      - DB_URL=postgresql://pi_3kd2_user:fbin7xBZxv5lkfP7gUJrpP2BtUbeyxRn@dpg-d44n3duuk2gs73fi8jag-a.oregon-postgres.render.com/pi_3kd2
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3000/health || curl -sf http://localhost:3000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    ports:
      - "127.0.0.1:3000:3000"   # Nginx (host) acessa aqui

  spark:
    image: iotkafkaacrwtvgek.azurecr.io/spark:latest
    container_name: spark-job
    restart: always
    depends_on:
      - nest
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - API_URL=http://nest-api:3000/data-process
      - SILO_CONF_URL=http://nest-api:3000/silos/conf
      - PORT=8080
    # sem publish; roda interno

volumes:
  redpanda_data:
  redis_data:
